---
title: "05 Bagging"
subtitle: "ML - Trees"
author: "Lasse Hyldig Hansen"
date: "2023-06-06"
output: 
  html_document:
    toc: true
---

## Bootstrap aggregation (“Bagging”)

Bootstrap aggregation, or “Bagging”, is another form of ensemble learning.

With boosting, we iteratively changed the dataset to have new trees focus on the “difficult” observations. Bagging involves the same approach, except we don’t selectively choose which observations to focus on, but rather we randomly select subsets of data each time.

Boosting aimed to iteratively improve our overall model with new trees. With bagging, we now build trees on what we hope are independent datasets.

Let’s take a step back, and think about a practical example. Say we wanted a good model of heart disease. If we saw researchers build a model from a dataset of patients from their hospital, we might think this would be sufficient. If the researchers were able to acquire a new dataset from new patients, and built a new model, we’d be inclined to feel that the combination of the two models would be better than any one individually.

This is the scenario that bagging aims to replicate, except instead of actually going out and collecting new datasets, we instead use “bootstrapping” to create new sets of data from our current dataset. If you are unfamiliar with bootstrapping, you can treat it as magic for now (and if you are familiar with the bootstrap, you already know that it is magic).

Let’s take a look at a simple bootstrap model:

```{r}
set.seed(321)

# Specify the model
library(rpart)
library(ipred)

# Create the bagging model
mdl <- bagging(actualhospitalmortality_enc ~ age + acutephysiologyscore, data = train_df, coob = TRUE, nbagg = 100, cp = 0.01)
```
## Visualizing Decision Boundaries 

```{r}
grid <- expand.grid(age = seq(min(train_df$age), max(train_df$age), length.out = 100),
                    acutephysiologyscore = seq(min(train_df$acutephysiologyscore), max(train_df$acutephysiologyscore), length.out = 100))

grid$prediction <- predict(mdl, newdata = grid)
  
# Create decision boundary plot

ggplot(grid, aes(x = age, y = acutephysiologyscore, fill = as.factor(prediction))) +
    geom_tile() +
    scale_fill_manual(values = c("blue", "red")) +
    labs(title = "Bagging Model with 100 Bags",
         x = "Age",
         y = "Acute Physiology Score",
         fill = "Predicted Class") +
    theme_classic()
```
## Changing number of bags

The number of these subsets, or 'bags', is a key parameter in a bagging algorithm. Adjusting this parameter affects the model's performance and behaviour, so it's important to understand how this works. In this case, we're investigating the effect of varying the number of 'bags' on the model's decision boundaries.

```{r}
library(ipred)
library(ggplot2)
library(gridExtra)

# Create a sequence of numbers of bags to try
n_bags <- c(10, 100, 500, 1000)

# Initialize a list to store the plots
plots <- list()

# Generate grid of points
grid <- expand.grid(age = seq(min(train_df$age), max(train_df$age), length.out = 100),
                    acutephysiologyscore = seq(min(train_df$acutephysiologyscore), max(train_df$acutephysiologyscore), length.out = 100))

# For each number of bags
for(i in seq_along(n_bags)) {
  # Create the bagging model
  mdl <- bagging(actualhospitalmortality_enc ~ age + acutephysiologyscore, data = train_df, coob = TRUE, nbagg = n_bags[i], cp = 0.01)
  
  # Predict class label for each point in the grid
  grid$prediction <- predict(mdl, newdata = grid)
  
  # Create decision boundary plot
  plots[[i]] <- ggplot(grid, aes(x = age, y = acutephysiologyscore, fill = as.factor(prediction))) +
    geom_tile() +
    scale_fill_manual(values = c("blue", "red")) +
    labs(title = paste0('Decision Boundary with ', n_bags[i], ' bags'),
         x = "Age",
         y = "Acute Physiology Score",
         fill = "Predicted Class") +
    theme_classic()
}

# Arrange the plots in a grid
gridExtra::grid.arrange(grobs = plots, ncol = 2)
```
By examining these plots, we can see how the decision boundaries change with the number of 'bags'. With fewer 'bags', the decision boundaries might be overly simplistic or irregular, indicating potential underfitting. As we increase the number of 'bags', the decision boundaries become more complex and better at distinguishing the classes. However, very complex boundaries could indicate overfitting, where the model fits the training data so closely that it may not perform well on unseen data.

The 'sweet spot' is a balance between underfitting and overfitting and can be different depending on the specific dataset and task.

Next up, a minor addition creates one of the most popular models in machine learning.

