---
title: "06 Random Forest"
subtitle: "ML - Trees"
author: "Lasse Hyldig Hansen"
date: "2023-06-06"
output: 
  html_document:
    toc: true
---

## Packages

```{r}
install.packages("randomForest")
library(randomForest)
library(ggplot2)
```
## Random Forest

In the previous example, we used bagging to randomly resample our data to generate “new” datasets. The Random Forest takes this one step further: instead of just resampling our data, we also select only a fraction of the features to include.

It turns out that this subselection tends to improve the performance of our models. The odds of an individual being very good or very bad is higher (i.e. the variance of the trees is increased), and this ends up giving us a final model with better overall performance (lower bias).

Let’s train the model.
## Training model

```{r}
# Set seed for reproducibility
set.seed(321)

# Train the Random Forest model
mdl <- randomForest(actualhospitalmortality_enc ~ age + acutephysiologyscore, data = train_df, ntree = 6, mtry = 1)
```


### Plotting decision boundary

```{r}
# Set seed for reproducibility
set.seed(321)

# Train the Random Forest model
mdl <- randomForest(actualhospitalmortality_enc ~ age + acutephysiologyscore, data = train_df, ntree = 6, mtry = 1)

# Generate grid of points
grid <- expand.grid(age = seq(min(train_df$age), max(train_df$age), length.out = 100),
                    acutephysiologyscore = seq(min(train_df$acutephysiologyscore), max(train_df$acutephysiologyscore), length.out = 100))

# Predict class label for each point in the grid
grid$prediction <- predict(mdl, newdata = grid)

# Create decision boundary plot for the final model
decision_boundary_plot_rf <- ggplot(grid, aes(x = age, y = acutephysiologyscore, fill = prediction)) +
  geom_tile() +
  scale_fill_manual(values = c("blue", "red")) +
  labs(title = "Random Forest",
       x = "Age",
       y = "Acute Physiology Score",
       fill = "Predicted Class") +
  theme_classic()

decision_boundary_plot_rf
```
## Seeing the effect of ntree (number of trees to grow)   

When we're developing machine learning models, it's important to understand how changing various parameters affects the model's behavior and performance. In this case, we're investigating the effect of the number of trees (ntree) in a Random Forest model on the model's decision boundaries. This is a critical parameter in Random Forest models, as it can significantly influence the accuracy and generalizability of the model. By visualizing the decision boundaries, we can gain a more intuitive understanding of how the model is classifying data points based on their features.

This exploration is part of the broader process of model tuning, where we experiment with different parameter settings to find the combination that gives the best performance for our specific task.

```{r}
library(randomForest)
library(ggplot2)
library(gridExtra)

# Set seed for reproducibility
set.seed(321)

# Create a sequence of numbers of trees to try
n_trees <- c(5, 10, 50, 100)

# Initialize a list to store the plots
plots <- list()

# Generate grid of points
grid <- expand.grid(age = seq(min(train_df$age), max(train_df$age), length.out = 100),
                    acutephysiologyscore = seq(min(train_df$acutephysiologyscore), max(train_df$acutephysiologyscore), length.out = 100))

# For each number of trees
for(i in seq_along(n_trees)) {
  # Train the Random Forest model
  mdl <- randomForest(actualhospitalmortality_enc ~ age + acutephysiologyscore, data = train_df, ntree = n_trees[i], mtry = 1)
  
  # Predict class label for each point in the grid
  grid$prediction <- predict(mdl, newdata = grid)
  
  # Create decision boundary plot
  plots[[i]] <- ggplot(grid, aes(x = age, y = acutephysiologyscore, fill = as.factor(prediction))) +
    geom_tile() +
    scale_fill_manual(values = c("blue", "red")) +
    labs(title = paste0('Random Forest (', n_trees[i], ' trees)'),
         x = "Age",
         y = "Acute Physiology Score",
         fill = "Predicted Class") +
    theme_classic()
}

# Arrange the plots in a grid
gridExtra::grid.arrange(grobs = plots, ncol = 2)
```
Each plot we've created represents the decision boundaries of a Random Forest model with a different number of trees, trained on the same dataset. The decision boundary is the threshold at which the model switches from predicting one class to predicting another.

By comparing these plots, we can observe how the decision boundaries change as we increase the number of trees in the model. With a small number of trees, the decision boundaries may appear somewhat simplistic or irregular, potentially indicating underfitting. As we increase the number of trees, we might notice the decision boundaries becoming more complex and possibly better at distinguishing between classes.

However, it's important to note that overly complex boundaries could be a sign of overfitting, where the model is so closely fitted to the training data that it may not generalize well to unseen data. The optimal number of trees will typically be the one that achieves a good balance between underfitting and overfitting. This optimal point can vary depending on the specific dataset and problem at hand.

Remember that these visualizations are for illustrative purposes and the true performance of the models should be evaluated using appropriate metrics and validation techniques, such as cross-validation.
