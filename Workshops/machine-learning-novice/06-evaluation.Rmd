---
title: "Evaluation"
author: "Kahina SEBBANE & Zahia YANES"
output: html_document
---

#### Teaching: 20

#### Exercises: 10

#### Questions:
- "What kind of values go into a confusion matrix?"
- "What do the letters AUROC stand for?"
- "Does an AUROC of 0.5 indicate our predictions were good, bad, or average?"
- "In the context of evaluating performance of a classifier, what is TP?"
 
#### Objectives:
- "Create a confusion matrix for a predictive model."
- "Use the confusion matrix to compute popular performance metrics."
- "Plot an AUROC curve."

## Evaluating a classification task

We trained a machine learning model to predict the outcome of patients admitted to intensive care units. As there are two outcomes, we refer to this as a “binary” classification task. We are now ready to evaluate the model on our held-out test set.

First we will load the packages needed:

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(caret)
library(glmnet)
library(tidyverse)
library(pheatmap)
library(ggplot2)
library(ROCR)
```

Then we will load the data, and partition it:

```{r}
# loading the dataframe into 'cohort'
cohort <- read.csv("../Data/eicu_cohort.csv")

# convert outcome to a categorical type
categories <- c("ALIVE", "EXPIRED")
cohort$actualhospitalmortality <- factor(cohort$actualhospitalmortality, levels = categories)

# add the encoded value to a new column
cohort$actualhospitalmortality_enc <- as.numeric(cohort$actualhospitalmortality)

# define features and outcome
features <- c("apachescore")
outcome <- c("actualhospitalmortality_enc")

# create ID column
cohort$id <- 1:nrow(cohort)

# partition data into training and test sets
  # use 70% of dataset as training set and 30% as test set 
train <- cohort %>% dplyr::sample_frac(0.70)
test  <- dplyr::anti_join(cohort, train, by = 'id')

# restructure data for input into model
  # make this example reproducible
set.seed(42)
x_test<-test$apachescore
y_test<- test$actualhospitalmortality
  # convert the ALIVE and EXPIRED categories to binary class labels
y_test_class <- ifelse(y_test=='ALIVE', 0,1)

# train model
model <- glm(actualhospitalmortality ~ apachescore , data = train, family = "binomial")

# generate predictions
y_hat_test_proba <- predict(model, newdata = test, type = "response")

# convert probabilities to binary class labels
y_hat_test_class <- ifelse(y_hat_test_proba >= 0.5, 1, 0)
```

Each prediction is assigned a probability of a positive class. For example, the first 22 probabilities are:

```{r}
probs <- y_hat_test_proba[1:22]
rounded_probs <- round(probs, 2)
print(rounded_probs)
```
These probabilities correspond to the following predictions, either a “0” (“ALIVE”) or a 1 (“EXPIRED”):

```{r}
print(y_hat_test_class[1:22])
```

In comparison with the known outcomes, we can put each prediction into one of the following categories:

- True positive: we predict “1” (“EXPIRED”) and the true outcome is “1”.
- True negative: we predict “0” (“ALIVE”) and the true outcome is “0”.
- False positive: we predict “1” (“EXPIRED”) and the true outcome is “0”.
- False negative: we predict “0” (“ALIVE”) and the true outcome is “1”.

```{r}
print ( y_test_class[1:22])
```

# Confusion matrices

It is common practice to arrange these outcome categories into a “confusion matrix”, which is a grid that records our predictions against the ground truth. For a binary outcome, confusion matrices are organized as follows:

||Negative (predicted)|Positive (predicted)|
|---------|--------|-------|
|Negative (actual)|  TN     |  FP    |
|Positive (actual)|  FN     |  TP    |

The sum of the cells is the total number of predictions. The diagonal from the top left to the bottom right indicates correct predictions. Let’s visualize the results of the model in the form of a confusion matrix:

```{r}
# converting y_hat_class and y_test_class in factors
y_hat_test_class <- factor(y_hat_test_class)
y_test_class <- factor(y_test_class)

# Calculating the confusion matrix
confusion <- confusionMatrix(y_hat_test_class, y_test_class)

# getting the class names
class_names <- levels(cohort$actualhospitalmortality)

# representation of the confusion matrix
cm <- as.table(confusion$table)
dimnames(cm) <- list(Actual = class_names, Predicted = class_names)
heatmap <- ggplot(data = as.data.frame(cm), aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile() +
  labs(title = "Confusion Matrix", x = "Predicted", y = "Actual") +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal()
```

Then we can print the confusion matrix:

```{r}
print(cm)
```

And the heatmap:

```{r}
print(heatmap)
```

We have two columns and rows because we have a binary outcome, but you can also extend the matrix to plot multi-class classification predictions. If we had more output classes, the number of columns and rows would match the number of classes.

# Accuracy
Accuracy is the overall proportion of correct predictions. Think of a dartboard. How many shots did we take? How many did we hit? Divide one by the other and that’s the accuracy.

Accuracy can be written as:

###                   $Accuracy=\frac{TP + TN}{TP+TN+FP+FN}$
What was the accuracy of our model?

```{r}
accuracy <- sum(y_hat_test_class == y_test_class) / length(y_test_class)
print(paste("Accuracy (model) =", round(accuracy, 2)))
```
Not bad at first glance. When comparing our performance to guessing “0” for every patient, however, it seems slightly less impressive!
```{r}
zeros <- rep(0, length(y_test_class))
acc <- sum(y_hat_test_class == zeros) / length(y_test_class)
print(paste("Accuracy (zeros) =", round(acc, 2)))

```
The problem with accuracy as a metric is that it is heavily influenced by prevalence of the positive outcome: because the proportion of 1s is relatively low, classifying everything as 0 is a safe bet.

We can see that the high accuracy is possible despite totally missing our target. To evaluate an algorithm in a way that prevalence does not cloud our assessment, we often look at sensitivity and specificity.

# Sensitivity (A.K.A “Recall” and “True Positive Rate”)

Sensitivity is the ability of an algorithm to predict a positive outcome when the actual outcome is positive. In our case, of the patients who die, what proportion did we correctly predict? This can be written as:

###                   $Sensitivity=Recall=\frac{TP}{TP+FN}$
Because a model that calls “1” for everything has perfect sensitivity, this measure is not enough on its own. Alongside sensitivity we often report on specificity.

# Specificity (A.K.A “True Negative Rate”)
Specificity relates to the test’s ability to correctly classify patients who survive their stay (i.e. class “0”). Specificity is the proportion of those who survive who are predicted to survive. The formula for specificity is:

###                   $Specificity=\frac{TN}{FP+TN}$

# Receiver-Operator Characteristic

A Receiver-Operator Characteristic (ROC) curve plots 1 - specificity vs. sensitivity at varying probability thresholds. The area under this curve is known as the AUROC (or sometimes just the “Area Under the Curve”, AUC) and it is a well-used measure of discrimination that was originally developed by radar operators in the 1940s.
```{r}
pred <-prediction(y_hat_test_proba,y_test_class)
perf <-performance(pred,"tpr","fpr") 
plot(perf, colorsize=TRUE)
```

An AUROC of 0.5 is no better than guessing and an AUROC of 1.0 is perfect. An AUROC of 0.9 tells us that the 90% of times our model will assign a higher risk to a randomly selected patient with an event than to a randomly selected patient without an event

