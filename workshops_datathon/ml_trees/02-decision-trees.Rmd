---
title: "02 - Decision Trees"
subtitle: "ML - Trees"
author: "Lasse Hyldig Hansen"
date: "2023-06-06"
output: 
  html_document:
    toc: true
---

## The simplest tree

Let's build the simplest tree model we can think of: a classification tree with only one split. Decision trees of this form are commonly referred to under the umbrella term Classification and Regression Trees (CART) [1].

While we will only be looking at classification here, regression isn't too different. After grouping the data (which is essentially what a decision tree does), classification involves assigning all members of the group to the majority class of that group during training. Regression is the same, except you would assign the average value, not the majority.

In the case of a decision tree with one split, often called a "stump", the model will partition the data into two groups, and assign classes for those two groups based on majority vote. There are many parameters available for the DecisionTreeClassifier class; by specifying max_depth=1 we will build a decision tree with only one split - i.e. of depth 1.

[1] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.

```{r}
# Install and load the required packages
install.packages("rpart")
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)

# Convert y_train to factor
y_train <- as.factor(y_train$actualhospitalmortality_enc)

# Create and fit the decision tree model
mdl <- rpart(y_train ~ age + acutephysiologyscore, data = x_train, method = "class", control = rpart.control(maxdepth = 1))

# Visualize the decision tree using rpart.plot
rpart.plot(mdl)
```


Here we see three nodes: a node at the top, a node in the lower left, and a node in the lower right.

The top node is the root of the tree: it contains all the data. 

The approach is referred to as "greedy" because we are choosing the optimal split given our current state. Let's take a closer look at our decision boundary.

```{r}
library(ggplot2)
library(gridExtra)
library(rpart)
library(rpart.plot)

# Generate grid of points
grid <- expand.grid(age = seq(min(train_df$age), max(train_df$age), length.out = 100),
                    acutephysiologyscore = seq(min(train_df$acutephysiologyscore), max(train_df$acutephysiologyscore), length.out = 100))

# Predict class label for each point in the grid
grid$prediction <- predict(mdl, newdata = grid, type = "class")

# Create decision boundary plot
decision_boundary_plot <- ggplot(grid, aes(x = age, y = acutephysiologyscore, fill = prediction)) +
  geom_point() +
  geom_tile() +
  scale_color_manual(values = c("blue", "red")) +
  labs(title = "Decision Boundary",
       x = "Age",
       y = "Acute Physiology Score",
       fill = "Predicted Class") +
  theme_classic()

decision_boundary_plot
```
In this plot we can see the decision boundary on the y-axis, separating the predicted classes. The true classes are indicated at each point. Where the background and point colours are mismatched, there has been misclassification. Of course we are using a very simple model.


